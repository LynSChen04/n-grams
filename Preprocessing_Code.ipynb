{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjQt2ZXm6uNA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Clones Detection\n",
        "\n",
        "\n",
        "Type-1 (Exact Clones):\n",
        "\n",
        "\t•\tCode snippets that are identical, except for differences in whitespace, comments, or formatting.\n",
        "\n",
        "Type-2 (Lexical Clones):\n",
        "\n",
        "\t•\tCode with the same structure but variations in elements such as variable names, function names, or literals, while preserving the overall logic.\n",
        "\n",
        "Type-3 (Syntactic Clones):\n",
        "\n",
        "\t•\tCode fragments with a similar structure that include modifications, such as added, removed, or altered statements, which may slightly affect the code flow.\n",
        "\n",
        "Type-4 (Semantic Clones):\n",
        "\n",
        "\t•\tCode snippets that perform the same functionality but are implemented using entirely different approaches, resulting in syntactically distinct code."
      ],
      "metadata": {
        "id": "pN8z0hbK7m9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Type 1 Clones ###\n",
        "def remove_duplicates(data):\n",
        "    \"\"\"Remove duplicate methods based on method content.\n",
        "      Almost Type-1 with the exception of comments\n",
        "    \"\"\"\n",
        "    return data.drop_duplicates(subset=\"Method Java\", keep=\"first\")"
      ],
      "metadata": {
        "id": "DaEH0BRa7Atw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_ascii_methods(data):\n",
        "    \"\"\"Filter methods to include only those with ASCII characters.\"\"\"\n",
        "    data = data[data[\"Method Java\"].apply(lambda x: all(ord(char) < 128 for char in x))]\n",
        "    return data"
      ],
      "metadata": {
        "id": "fLsSvHNe81B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Three Approaches:\n",
        "# \t1.\tData Distribution-Based Filtering: We eliminate outliers by analyzing the original data distribution, as demonstrated below.\n",
        "# \t2.\tLiterature-Driven Filtering: We follow best practices outlined in research, such as removing methods exceeding 512 tokens in length.\n",
        "# \t3.\tHybrid Approach: We combine elements from both the distribution-based and literature-driven methods.\n",
        "\n",
        "def remove_outliers(data, lower_percentile=5, upper_percentile=95):\n",
        "    \"\"\"Remove outliers based on method length.\"\"\"\n",
        "    method_lengths = data[\"Method Java\"].apply(len)\n",
        "    lower_bound = method_lengths.quantile(lower_percentile / 100)\n",
        "    upper_bound = method_lengths.quantile(upper_percentile / 100)\n",
        "    return data[(method_lengths >= lower_bound) & (method_lengths <= upper_bound)]"
      ],
      "metadata": {
        "id": "AADtuKbI9p1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Tokens\n",
        "Usually, to represent the information relevant to a data-driven technique, we try to provide textual data as input to the model.\n",
        "In this context, we can define\n",
        "\n",
        "Code Tokens as, the smallest meaningful units of a programming language, derived from parsing the source code. These include keywords, operators, identifiers, literals, and punctuation marks. Tokens are the building blocks of code that represent its syntactic structure.\n",
        "\n",
        "`public String greet(String name) {\n",
        "    return \"Hello, \" + name + \"!\";\n",
        "}`\n",
        "\n",
        "Code Tokens:\n",
        "\n",
        "\t1.\tpublic\n",
        "\t2.\tString\n",
        "\t3.\tgreet\n",
        "\t4.\tString\n",
        "\t5.\tname\n",
        "\t6.\treturn\n",
        "\t7.\t\"Hello, \"\n",
        "\t8.\t+\n",
        "\t9.\tname\n",
        "\t10.\t+\n",
        "\t11.\t\"!\"\n",
        "\n",
        "Code tokens can potentially exclude punctuation like (, ), {, }, ; and focus on the meaningful components of the code logic.\n",
        "\n",
        "### What is the relationship between Code Tokens and Tokenizer(s)?\n",
        "\n",
        "A tokenizer is a tool or algorithm that breaks down source code (generally speaking text) into its smallest meaningful components, called tokens. These tokens are extracted from the raw code and represent syntactic and semantic elements such as keywords, operators, identifiers, literals, and punctuation.\n",
        "\n",
        "\n",
        "### Why we rely on them? ###\n",
        "Tokens are one of the easiest and most effective ways to transmit information to a machine learning model, especially in the context of code understanding and generation. Treating code at the token level provides a unified and **text-based** representation that is both human-readable and computationally efficient."
      ],
      "metadata": {
        "id": "fdDrizky-kEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a Vocabulary in this context?\n",
        "In the context of token-based representation for data-driven AI models, a vocabulary refers to the complete set of unique tokens that the model is capable of understanding and processing. One of the key role of the vocubalary is to map between tokens (e.g., keywords, identifiers, operators, punctuation, etc.) and their corresponding numerical representations, which are used as inputs to the model.\n",
        "\n",
        "\n",
        "Let's construct the set of uniq tokens of the dataset we came up with!"
      ],
      "metadata": {
        "id": "gy3zqpJgA_hC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization with Pygments"
      ],
      "metadata": {
        "id": "gmhFPBScCMDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pygments.lexers.jvm import JavaLexer\n",
        "from pygments.lexers import get_lexer_by_name\n",
        "from pygments.token import Token\n",
        "\n",
        "code = \"\"\"public static void main() { System.out.println(\"bau\");}\"\"\"\n",
        "\n",
        "lexer = JavaLexer()\n",
        "\n",
        "tokens = [t[1] for t in lexer.get_tokens(code)]\n",
        "print(tokens)\n",
        "print(len(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1lq3t0JFh2m",
        "outputId": "0acf8e33-44f5-4938-9e62-d9789496ef78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['public', ' ', 'static', ' ', 'void', ' ', 'main', '(', ')', ' ', '{', ' ', 'System', '.', 'out', '.', 'println', '(', '\"', 'bau', '\"', ')', ';', '}', '\\n']\n",
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_boilerplate_methods(data):\n",
        "    \"\"\"Remove boilerplate methods like setters and getters.\"\"\"\n",
        "    boilerplate_patterns = [\n",
        "        r\"\\bset[A-Z][a-zA-Z0-9_]*\\(.*\\)\\s*{\",  # Setter methods\n",
        "        r\"\\bget[A-Z][a-zA-Z0-9_]*\\(.*\\)\\s*{\",  # Getter methods\n",
        "    ]\n",
        "    boilerplate_regex = re.compile(\"|\".join(boilerplate_patterns))\n",
        "    data = data[~data[\"Method Java\"].apply(lambda x: bool(boilerplate_regex.search(x)))]\n",
        "    return data\n",
        "\n",
        "\n",
        "def remove_comments_from_dataframe(df: pd.DataFrame, method_column: str, language: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes comments from Java methods in a DataFrame and adds a new column with cleaned methods.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing the methods.\n",
        "        method_column (str): Column name containing the raw Java methods.\n",
        "        language (str): Programming language for the lexer (e.g., 'java').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Updated DataFrame with a new column 'Java Method No Comments'.\n",
        "    \"\"\"\n",
        "    # Define a function to remove comments from a single method\n",
        "    def remove_comments(code):\n",
        "        lexer = get_lexer_by_name(language)\n",
        "        tokens = lexer.get_tokens(code)\n",
        "        # Filter out comments using a lambda function\n",
        "        clean_code = ''.join(token[1] for token in tokens if not (lambda t: t[0] in Token.Comment)(token))\n",
        "\n",
        "\n",
        "        return clean_code\n",
        "\n",
        "    # Apply the function to the specified column and add a new column with the results\n",
        "    df[\"Method Java No Comments\"] = df[method_column].apply(remove_comments)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Example usage\n",
        "data = pd.DataFrame({\n",
        "    \"Method Java\": [\n",
        "        \"public void setName(String name) { this.name = name; }\",\n",
        "        \"public String getName() { return this.name; }\",\n",
        "        \"public void processData() { System.out.println(\\\"Processing data\\\"); }\",\n",
        "        \"// This is a comment\\npublic void processData() { /* Do something */ System.out.println(\\\"Done\\\"); }\",\n",
        "        \"public void doWork() { for(int i=0; i<10; i++) /* Do something */ System.out.println(i); }\",\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Initial dataset size:\", len(data))\n",
        "data = remove_duplicates(data)\n",
        "print(\"After removing duplicates:\", len(data))\n",
        "\n",
        "data = filter_ascii_methods(data)\n",
        "print(\"After filtering ASCII methods:\", len(data))\n",
        "\n",
        "data = remove_outliers(data)\n",
        "print(\"After removing outliers:\", len(data))\n",
        "\n",
        "data = remove_boilerplate_methods(data)\n",
        "print(\"After removing boilerplate methods:\", len(data))\n",
        "\n",
        "data = remove_comments_from_dataframe(data, \"Method Java\", \"Java\")\n",
        "print(\"After cleaning comments:\", len(data))\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "5YSANlSI6zB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vC833CT_84rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MB0N-cgj6_T9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}